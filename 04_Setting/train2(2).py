# =================================================================
# Fashion MNIST V2 - 95% Training Accuracy ë‹¬ì„± + Test Accuracy ìµœëŒ€í™”
# =================================================================
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš©
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# =================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° 3ë‹¨ê³„ ë¶„ë¦¬
# =================================================================
fashion_mnist = keras.datasets.fashion_mnist
(train_val_images, train_val_labels), (test_images_orig, test_labels_orig) = fashion_mnist.load_data()

class_names = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot",
]

# ì „ì²´ ë°ì´í„° ì¬ë¶„ë°°
all_images = np.concatenate([train_val_images, test_images_orig])
all_labels = np.concatenate([train_val_labels, test_labels_orig])

train_val_images, test_images, train_val_labels, test_labels = train_test_split(
    all_images, all_labels, test_size=0.4, random_state=42
)

train_images, val_images, train_labels, val_labels = train_test_split(
    train_val_images, train_val_labels, test_size=0.16666, random_state=42
)

# =================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ + Data Augmentation
# =================================================================
train_images = np.expand_dims(train_images / 255.0, -1)
val_images = np.expand_dims(val_images / 255.0, -1)
test_images = np.expand_dims(test_images / 255.0, -1)

# ê°•ë ¥í•œ Data Augmentation
data_augmentation = keras.Sequential([
    layers.RandomRotation(0.15),        # Â±15ë„ íšŒì „
    layers.RandomTranslation(0.1, 0.1), # 10% ì´ë™
    layers.RandomZoom(0.1),             # 10% ì¤Œ
    layers.RandomContrast(0.1),         # ëŒ€ë¹„ ì¡°ì •
], name="data_augmentation")

# =================================================================
# 3. í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì½œë°±
# =================================================================
class TestMetricsCallback(keras.callbacks.Callback):
    def __init__(self, test_data):
        self.test_data = test_data
        self.test_accs = []
        self.test_losses = []
        self.best_test_acc = 0.0

    def on_epoch_end(self, epoch, logs=None):
        x_test, y_test = self.test_data
        loss, acc = self.model.evaluate(x_test, y_test, verbose=0)
        self.test_losses.append(loss)
        self.test_accs.append(acc)
        
        if acc > self.best_test_acc:
            self.best_test_acc = acc
            print(f" â€” test_loss: {loss:.4f} â€” test_acc: {acc:.4f} â­NEW BEST!")
        else:
            print(f" â€” test_loss: {loss:.4f} â€” test_acc: {acc:.4f}")

test_callback = TestMetricsCallback((test_images, test_labels))

# =================================================================
# 4. ê³ ì„±ëŠ¥ CNN ëª¨ë¸ (95% Training ëª©í‘œ)
# =================================================================
def create_high_performance_model():
    inputs = keras.Input(shape=(28, 28, 1))
    
    # Data Augmentation
    x = data_augmentation(inputs)
    
    # Block 1: 64 filters
    x = layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.25)(x)
    
    # Block 2: 128 filters
    x = layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.25)(x)
    
    # Block 3: 256 filters
    x = layers.Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.5)(x)
    
    # Dense layers
    x = layers.Dense(512, kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(0.5)(x)
    
    x = layers.Dense(256, kernel_initializer='he_normal')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Dropout(0.3)(x)
    
    outputs = layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')(x)
    
    return keras.Model(inputs, outputs, name="high_performance_fashion_cnn")

# ëª¨ë¸ ìƒì„±
model = create_high_performance_model()

# ìµœì í™”ëœ ì»´íŒŒì¼
model.compile(
    optimizer=keras.optimizers.AdamW(
        learning_rate=0.001,
        weight_decay=1e-4,
        beta_1=0.9,
        beta_2=0.999
    ),
    loss=keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),
    metrics=['accuracy']
)

print("ğŸš€ High-Performance Model Architecture:")
model.summary()
print(f"ğŸ“Š Total parameters: {model.count_params():,}")

# =================================================================
# 5. ê³ ê¸‰ í•™ìŠµ ì „ëµ
# =================================================================
# Cosine Annealing with Warm Restart
def cosine_annealing_with_warmup(epoch):
    """Cosine annealing with warm restart"""
    T_0 = 20  # ì²« ë²ˆì§¸ restart ì£¼ê¸°
    T_mult = 1.2  # restart ì£¼ê¸° ë°°ìˆ˜
    eta_max = 0.001  # ìµœëŒ€ í•™ìŠµë¥ 
    eta_min = 1e-6   # ìµœì†Œ í•™ìŠµë¥ 
    
    if epoch < 5:  # Warm-up
        return eta_max * epoch / 5
    else:
        T_cur = epoch - 5
        return eta_min + (eta_max - eta_min) * (1 + np.cos(np.pi * (T_cur % T_0) / T_0)) / 2

# ì½œë°± ì„¤ì •
callbacks = [
    test_callback,
    
    # Learning Rate Scheduling
    keras.callbacks.LearningRateScheduler(cosine_annealing_with_warmup, verbose=1),
    
    # Model Checkpoint (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥)
    keras.callbacks.ModelCheckpoint(
        'best_fashion_model.h5',
        monitor='test_acc' if hasattr(test_callback, 'test_accs') else 'val_accuracy',
        save_best_only=True,
        verbose=1
    ),
    
    # Early Stopping (ë§¤ìš° ê´€ëŒ€í•˜ê²Œ ì„¤ì •)
    keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    
    # Plateau ê°ì§€ ì‹œ í•™ìŠµë¥  ê°ì†Œ
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_accuracy',
        factor=0.7,
        patience=7,
        min_lr=1e-7,
        verbose=1
    ),
]

# =================================================================
# 6. ëª¨ë¸ í•™ìŠµ (95% ë‹¬ì„± ëª©í‘œ)
# =================================================================
print("\nğŸ¯ ëª©í‘œ: Training Accuracy 95% + Test Accuracy ìµœëŒ€í™”")
print("=" * 60)

history = model.fit(
    train_images, train_labels,
    epochs=80,  # ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„
    batch_size=64,
    validation_data=(val_images, val_labels),
    callbacks=callbacks,
    verbose=1
)

print("\nâœ… í•™ìŠµ ì™„ë£Œ!")

# =================================================================
# 7. ìƒì„¸ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
# =================================================================
acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
test_acc = test_callback.test_accs
loss = history.history["loss"]
val_loss = history.history["val_loss"]
test_loss = test_callback.test_losses

epochs_range = range(len(acc))

# í° í™”ë©´ìœ¼ë¡œ ì‹œê°í™”
plt.figure(figsize=(20, 8))

# ì •í™•ë„ ê·¸ë˜í”„
plt.subplot(2, 3, 1)
plt.plot(epochs_range, acc, 'b-', label="Training", linewidth=3, alpha=0.8)
plt.plot(epochs_range, val_acc, 'r-', label="Validation", linewidth=3, alpha=0.8)
plt.plot(epochs_range, test_acc, 'g-', label="Test", linewidth=3, alpha=0.8)
plt.axhline(y=0.95, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='95% Goal')
plt.legend(loc="lower right", fontsize=12)
plt.title("Accuracy Progress", fontsize=16, fontweight='bold')
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.grid(True, alpha=0.3)
plt.ylim([0.7, 1.0])

# ì†ì‹¤ ê·¸ë˜í”„
plt.subplot(2, 3, 2)
plt.plot(epochs_range, loss, 'b-', label="Training", linewidth=3, alpha=0.8)
plt.plot(epochs_range, val_loss, 'r-', label="Validation", linewidth=3, alpha=0.8)
plt.plot(epochs_range, test_loss, 'g-', label="Test", linewidth=3, alpha=0.8)
plt.legend(loc="upper right", fontsize=12)
plt.title("Loss Progress", fontsize=16, fontweight='bold')
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Loss", fontsize=12)
plt.grid(True, alpha=0.3)

# ì˜¤ë²„í”¼íŒ… ë¶„ì„
plt.subplot(2, 3, 3)
train_test_diff = np.array(acc) - np.array(test_acc)
plt.plot(epochs_range, train_test_diff, 'purple', linewidth=3, alpha=0.8, label="Train-Test Gap")
plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
plt.axhline(y=0.05, color='red', linestyle=':', alpha=0.7, label='5% Gap')
plt.axhline(y=0.10, color='orange', linestyle=':', alpha=0.7, label='10% Gap')
plt.legend(fontsize=12)
plt.title("Overfitting Analysis", fontsize=16, fontweight='bold')
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Accuracy Gap", fontsize=12)
plt.grid(True, alpha=0.3)

# í•™ìŠµë¥  ê·¸ë˜í”„
if 'lr' in history.history:
    plt.subplot(2, 3, 4)
    plt.plot(epochs_range, history.history['lr'], 'orange', linewidth=3, alpha=0.8)
    plt.title("Learning Rate Schedule", fontsize=16, fontweight='bold')
    plt.xlabel("Epoch", fontsize=12)
    plt.ylabel("Learning Rate", fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

# ì„±ëŠ¥ ìš”ì•½
plt.subplot(2, 3, 5)
final_metrics = [max(acc), max(val_acc), max(test_acc)]
labels = ['Train\n(Best)', 'Validation\n(Best)', 'Test\n(Best)']
colors = ['blue', 'red', 'green']
bars = plt.bar(labels, final_metrics, color=colors, alpha=0.7, edgecolor='black')
plt.axhline(y=0.95, color='orange', linestyle='--', linewidth=2, alpha=0.7)
plt.title("Final Performance", fontsize=16, fontweight='bold')
plt.ylabel("Accuracy", fontsize=12)
plt.ylim([0.85, 1.0])

# ê° ë°” ìœ„ì— ì •í™•í•œ ìˆ˜ì¹˜ í‘œì‹œ
for i, (bar, metric) in enumerate(zip(bars, final_metrics)):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
             f'{metric:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

# =================================================================
# 8. ìµœì¢… ê²°ê³¼ ë³´ê³ ì„œ
# =================================================================
max_train_acc = max(acc)
max_val_acc = max(val_acc)
max_test_acc = max(test_acc)
final_gap = max_train_acc - max_test_acc

print("\n" + "="*80)
print("ğŸ† FINAL PERFORMANCE REPORT")
print("="*80)
print(f"ğŸ¯ Training Accuracy (Max):     {max_train_acc:.4f} {'âœ… GOAL ACHIEVED!' if max_train_acc >= 0.95 else 'âŒ Below 95%'}")
print(f"ğŸ“Š Validation Accuracy (Max):   {max_val_acc:.4f}")
print(f"ğŸ… Test Accuracy (Max):         {max_test_acc:.4f}")
print(f"ğŸ“‰ Training-Test Gap:           {final_gap:.4f}")
print(f"ğŸ² Best Test Accuracy:          {test_callback.best_test_acc:.4f}")
print("="*80)

# ì„±ê³µ ê¸°ì¤€ í‰ê°€
if max_train_acc >= 0.95:
    print("ğŸ‰ SUCCESS: Training accuracy target achieved!")
    print(f"ğŸ’ª Test Performance: {max_test_acc:.1%}")
    
    if final_gap <= 0.05:
        print("ğŸŒŸ EXCELLENT: Low overfitting (â‰¤5% gap)")
    elif final_gap <= 0.10:
        print("âœ… GOOD: Moderate overfitting (â‰¤10% gap)")
    else:
        print("âš ï¸ WARNING: High overfitting (>10% gap)")
else:
    print("ğŸ“ˆ Need more training or model capacity for 95% target")

print(f"\nğŸ“‹ Model Summary:")
print(f"   â€¢ Parameters: {model.count_params():,}")
print(f"   â€¢ Architecture: Deep CNN (3 blocks)")
print(f"   â€¢ Regularization: Dropout + BatchNorm + L2")
print(f"   â€¢ Data Augmentation: âœ…")
print(f"   â€¢ Label Smoothing: âœ…")

# =================================================================
# 9. ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼
# =================================================================
predictions = model.predict(test_images)

def plot_prediction_sample():
    plt.figure(figsize=(16, 10))
    
    for i in range(12):
        plt.subplot(3, 4, i + 1)
        
        img = test_images[i].squeeze()
        true_label = test_labels[i]
        pred_probs = predictions[i]
        pred_label = np.argmax(pred_probs)
        confidence = np.max(pred_probs)
        
        plt.imshow(img, cmap='gray')
        plt.axis('off')
        
        color = 'green' if pred_label == true_label else 'red'
        plt.title(f'True: {class_names[true_label]}\nPred: {class_names[pred_label]}\nConf: {confidence:.2f}', 
                 color=color, fontsize=10, fontweight='bold')
    
    plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

plot_prediction_sample()

print("\nğŸš€ Training Complete! Check the graphs above for detailed analysis.")